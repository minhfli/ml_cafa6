{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":116062,"databundleVersionId":14875579,"sourceType":"competition"},{"sourceId":14141413,"sourceType":"datasetVersion","datasetId":9012019}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Logistic Regression with ESM-2 embeddings\n\nIdea:\n- Model: OneVsRestClassifier(LogisticRegression) --> train one Logistic Regression model for one class\n- Features: PCA(n_components=100) --> PCA.fit_transform(ESM-2 embeddings)\n- Labels: Three sets for three ontologies (P, C, F)\n    - P has 16858 classes\n    - C has 2651 classes\n    - F has 6616 classes\n\nFuture direction:\n- Model --> MLP\n- Features --> full embeddings\n\nReferences:\n- (EDA + OneVsRestClassifier) https://www.kaggle.com/code/analyticaobscura/cafa-6-decoding-protein-mysteries\n- (ESM-2 320-D embeddings) https://www.kaggle.com/code/dalloliogm/compute-protein-embeddings-with-esm2-esm-c/notebook\n- (Optional ProtT5 1024-D embeddings) https://www.kaggle.com/code/ahsuna123/t5-embedding-calculation-cafa-6/output?select=train_ids.npy\n\n---","metadata":{}},{"cell_type":"code","source":"!pip install biopython > /dev/null","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T13:25:01.048190Z","iopub.execute_input":"2025-12-13T13:25:01.048537Z","iopub.status.idle":"2025-12-13T13:25:08.639777Z","shell.execute_reply.started":"2025-12-13T13:25:01.048510Z","shell.execute_reply":"2025-12-13T13:25:08.638678Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Step 1: Load CAFA6 files\n\n---","metadata":{}},{"cell_type":"code","source":"# CAFA6 file paths\nTRAIN_TERMS = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\"\nTRAIN_SEQ = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta\"\nTEST_SEQ = \"/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T13:25:08.641900Z","iopub.execute_input":"2025-12-13T13:25:08.642190Z","iopub.status.idle":"2025-12-13T13:25:08.646536Z","shell.execute_reply.started":"2025-12-13T13:25:08.642161Z","shell.execute_reply":"2025-12-13T13:25:08.645811Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from Bio import SeqIO \n\n# Dict {entryId, seq}\ntrain_sequences = {rec.id: str(rec.seq) for rec in SeqIO.parse(TRAIN_SEQ, 'fasta')}\ntest_sequences  = {rec.id: str(rec.seq) for rec in SeqIO.parse(TEST_SEQ,  'fasta')}\n\nprint(f'Loaded {len(train_sequences)} train and {len(test_sequences)} test sequences')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T13:25:08.647359Z","iopub.execute_input":"2025-12-13T13:25:08.647641Z","iopub.status.idle":"2025-12-13T13:25:11.419631Z","shell.execute_reply.started":"2025-12-13T13:25:08.647614Z","shell.execute_reply":"2025-12-13T13:25:11.418853Z"}},"outputs":[{"name":"stdout","text":"Loaded 82404 train and 224309 test sequences\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"print(\"Train dict:\", list(train_sequences.items())[0])\nprint(\"Test dict:\", list(test_sequences.items())[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T13:25:11.420649Z","iopub.execute_input":"2025-12-13T13:25:11.420931Z","iopub.status.idle":"2025-12-13T13:25:11.499220Z","shell.execute_reply.started":"2025-12-13T13:25:11.420898Z","shell.execute_reply":"2025-12-13T13:25:11.498329Z"}},"outputs":[{"name":"stdout","text":"Train dict: ('sp|A0A0C5B5G6|MOTSC_HUMAN', 'MRWQEMGYIFYPRKLR')\nTest dict: ('A0A0C5B5G6', 'MRWQEMGYIFYPRKLR')\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"train_ids = [i.split('|')[1] for i in train_sequences.keys()]\ntest_ids = list(test_sequences.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T13:25:11.501597Z","iopub.execute_input":"2025-12-13T13:25:11.501955Z","iopub.status.idle":"2025-12-13T13:25:11.537448Z","shell.execute_reply.started":"2025-12-13T13:25:11.501931Z","shell.execute_reply":"2025-12-13T13:25:11.536620Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"print(\"train_ids[0:10]:\", train_ids[0:10])\nprint(\"test_ids[0:10]:\", test_ids[0:10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T13:25:11.538245Z","iopub.execute_input":"2025-12-13T13:25:11.538494Z","iopub.status.idle":"2025-12-13T13:25:11.552908Z","shell.execute_reply.started":"2025-12-13T13:25:11.538474Z","shell.execute_reply":"2025-12-13T13:25:11.552039Z"}},"outputs":[{"name":"stdout","text":"train_ids[0:10]: ['A0A0C5B5G6', 'A0JNW5', 'A0JP26', 'A0PK11', 'A1A4S6', 'A1A519', 'A1L190', 'A1L3X0', 'A1X283', 'A2A2Y4']\ntest_ids[0:10]: ['A0A0C5B5G6', 'A0A1B0GTW7', 'A0JNW5', 'A0JP26', 'A0PK11', 'A1A4S6', 'A1A519', 'A1L190', 'A1L3X0', 'A1X283']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Step 2: Feature extraction\n\n---","metadata":{}},{"cell_type":"code","source":"# Embeddings file paths\nESM_EMBEDDINGS = \"/kaggle/input/cafa6-esm2-650m-embedding/esm2_650M\"\nTRAIN_EMBEDDINGS = ESM_EMBEDDINGS + \"/train_sequences_emb.npy\"\nTEST_EMBEDDINGS = ESM_EMBEDDINGS + \"/testsuperset_emb.npy\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T13:25:11.554022Z","iopub.execute_input":"2025-12-13T13:25:11.554239Z","iopub.status.idle":"2025-12-13T13:25:11.569862Z","shell.execute_reply.started":"2025-12-13T13:25:11.554221Z","shell.execute_reply":"2025-12-13T13:25:11.569106Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import numpy as np\n\n# Load embeddings\nX_train = np.load(TRAIN_EMBEDDINGS)\nX_test = np.load(TEST_EMBEDDINGS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T13:25:11.570760Z","iopub.execute_input":"2025-12-13T13:25:11.571198Z","iopub.status.idle":"2025-12-13T13:25:15.192563Z","shell.execute_reply.started":"2025-12-13T13:25:11.571173Z","shell.execute_reply":"2025-12-13T13:25:15.191620Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"print(\"X_train shape:\", X_train.shape)\nprint(\"X_test shape:\", X_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T13:25:15.193517Z","iopub.execute_input":"2025-12-13T13:25:15.193826Z","iopub.status.idle":"2025-12-13T13:25:15.198903Z","shell.execute_reply.started":"2025-12-13T13:25:15.193794Z","shell.execute_reply":"2025-12-13T13:25:15.197938Z"}},"outputs":[{"name":"stdout","text":"X_train shape: (82404, 1280)\nX_test shape: (224309, 1280)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=100, random_state=42)\nX_train_reduced = pca.fit_transform(X_train)\nX_test_reduced  = pca.transform(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T13:25:15.199739Z","iopub.execute_input":"2025-12-13T13:25:15.199985Z","iopub.status.idle":"2025-12-13T13:25:32.417465Z","shell.execute_reply.started":"2025-12-13T13:25:15.199967Z","shell.execute_reply":"2025-12-13T13:25:32.416426Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"print(\"X_train_reduced shape:\", X_train_reduced.shape)\nprint(\"X_test_reduced shape:\", X_test_reduced.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T13:25:32.418452Z","iopub.execute_input":"2025-12-13T13:25:32.418809Z","iopub.status.idle":"2025-12-13T13:25:32.423472Z","shell.execute_reply.started":"2025-12-13T13:25:32.418789Z","shell.execute_reply":"2025-12-13T13:25:32.422631Z"}},"outputs":[{"name":"stdout","text":"X_train_reduced shape: (82404, 100)\nX_test_reduced shape: (224309, 100)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Step 3: Label encoding and Training\n\n---","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom tqdm import tqdm\n\nmlb_dict = dict()\nmodels = dict()\n\ntrain_terms_df = pd.read_csv(TRAIN_TERMS, sep=\"\\t\")\n\nfor aspect in tqdm(['P', 'C', 'F'], desc=\"Training models\"):\n    # Filter the train_terms_df based on aspect\n    ont_terms_df = train_terms_df[train_terms_df['aspect'] == aspect]\n\n    # Group the dataFrame based on the EntryID, turn all the GO terms to a list, finally turns this dataFrame to a dict {entryID: terms}\n    protein_terms = ont_terms_df.groupby('EntryID')['term'].apply(list).to_dict()\n\n    # Create a list of labels for this aspect, if an entryID doesn't exist in this aspect, give it a []\n    # This ensures y_train is of shape (82404, ...)\n    labels = [protein_terms.get(entry_id, []) for entry_id in train_ids]\n\n    # Multi-hot encoding, use sparse representation\n    mlb = MultiLabelBinarizer(sparse_output=True)\n    y_train = mlb.fit_transform(labels)\n    \n    print(f\"y_train shape for {aspect} ontology: {y_train.shape} \\t Number of unique {aspect} terms: {y_train.shape[1]}\")\n\n    # Save to dict\n    mlb_dict[aspect] = mlb\n    model = OneVsRestClassifier(LogisticRegression(max_iter=600, solver='lbfgs', C=0.5, random_state=42), n_jobs=-1)\n    model.fit(X_train_reduced, y_train)\n    models[aspect] = model\n    print(f\"Model for {aspect} trained successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T13:25:32.424089Z","iopub.execute_input":"2025-12-13T13:25:32.424323Z","iopub.status.idle":"2025-12-13T13:33:00.439649Z","shell.execute_reply.started":"2025-12-13T13:25:32.424304Z","shell.execute_reply":"2025-12-13T13:33:00.438206Z"}},"outputs":[{"name":"stderr","text":"Training models:   0%|          | 0/3 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"y_train shape for P ontology: (82404, 16858) \t Number of unique P terms: 16858\n","output_type":"stream"},{"name":"stderr","text":"Training models:   0%|          | 0/3 [07:27<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/146474223.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mmlb_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneVsRestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_reduced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model for {aspect} trained successfully.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# n_jobs > 1 in can results in slower performance due to the overhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# of spawning threads.  See joblib issue #112.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         self.estimators_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(\n\u001b[0m\u001b[1;32m    331\u001b[0m             delayed(_fit_binary)(\n\u001b[1;32m    332\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":12},{"cell_type":"markdown","source":"## Step 4: Inference and Submission","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 5000  # avoid memory overflow\nsubmission_list = []\n\nfor i in tqdm(range(0, len(test_ids), BATCH_SIZE), desc=\"Predicting on Test Set\"):\n    batch_entry_ids = test_ids[i : i + BATCH_SIZE]\n\n    X_batch = X_test_reduced[i : i + BATCH_SIZE]\n\n    for aspect, model in models.items():\n        mlb = mlb_dict[aspect]\n        y_pred_proba = model.predict_proba(X_batch)\n\n        for j, entry_id in enumerate(batch_entry_ids):\n            probs = y_pred_proba[j]\n            candidate_indices = np.where(probs > 0.02)[0]\n\n            for idx in candidate_indices:\n                submission_list.append((entry_id, mlb.classes_[idx], round(probs[idx], 3)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T13:33:00.440485Z","iopub.status.idle":"2025-12-13T13:33:00.440838Z","shell.execute_reply.started":"2025-12-13T13:33:00.440699Z","shell.execute_reply":"2025-12-13T13:33:00.440714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df = pd.DataFrame(submission_list, columns=['Protein Id', 'GO Term Id', 'Prediction'])\nsubmission_df.to_csv('submission_no_limit.tsv', sep='\\t', index=False, header=False)\n\nprint(\"Applying 1500 prediction limit per protein...\")\nsubmission_df = submission_df.sort_values(by=['Protein Id', 'Prediction'], ascending=[True, False])\nfinal_submission_df = submission_df.groupby('Protein Id').head(1500).reset_index(drop=True)\nfinal_submission_df.to_csv('submission.tsv', sep='\\t', index=False, header=False)\n\nprint(\"\\nSubmission file 'submission.tsv' created successfully.\")\nprint(f\"Total predictions in final submission: {len(final_submission_df):,}\")\nprint(\"Submission DataFrame Head:\")\ndisplay(final_submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T13:33:00.442290Z","iopub.status.idle":"2025-12-13T13:33:00.442662Z","shell.execute_reply.started":"2025-12-13T13:33:00.442477Z","shell.execute_reply":"2025-12-13T13:33:00.442493Z"}},"outputs":[],"execution_count":null}]}