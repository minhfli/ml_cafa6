{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":116062,"databundleVersionId":14875579,"sourceType":"competition"},{"sourceId":13991224,"sourceType":"datasetVersion","datasetId":8917191},{"sourceId":14141413,"sourceType":"datasetVersion","datasetId":9012019}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LightGBM with ESM-2 embeddings\n\nIdea:\n- Model: OneVsRestClassifier(LightGBM) --> train one Logistic Regression model for one class\n- Features: PCA(n_components=100) --> PCA.fit_transform(ESM-2 embeddings)\n- Labels: Three sets for three ontologies (P, C, F)\n    - P has 16858 classes\n    - C has 2651 classes\n    - F has 6616 classes\n- Only top 1000 most frequent class in each ontology are used to train\n\nReferences:\n- (EDA + OneVsRestClassifier) https://www.kaggle.com/code/analyticaobscura/cafa-6-decoding-protein-mysteries\n- (ESM-2 320-D embeddings) https://www.kaggle.com/code/dalloliogm/compute-protein-embeddings-with-esm2-esm-c/notebook\n- (Optional ProtT5 1024-D embeddings) https://www.kaggle.com/code/ahsuna123/t5-embedding-calculation-cafa-6/output?select=train_ids.npy\n\n---","metadata":{}},{"cell_type":"code","source":"!pip install biopython > /dev/null","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:09:03.469312Z","iopub.execute_input":"2025-12-14T06:09:03.469478Z","iopub.status.idle":"2025-12-14T06:09:08.990915Z","shell.execute_reply.started":"2025-12-14T06:09:03.469462Z","shell.execute_reply":"2025-12-14T06:09:08.990000Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 1: Load CAFA6 files\n\n---","metadata":{}},{"cell_type":"code","source":"# CAFA6 file paths\nTRAIN_TERMS = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\"\nTRAIN_SEQ = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta\"\nTEST_SEQ = \"/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:09:08.993203Z","iopub.execute_input":"2025-12-14T06:09:08.993493Z","iopub.status.idle":"2025-12-14T06:09:08.997495Z","shell.execute_reply.started":"2025-12-14T06:09:08.993466Z","shell.execute_reply":"2025-12-14T06:09:08.996812Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from Bio import SeqIO \n\n# Dict {entryId, seq}\ntrain_sequences = {rec.id: str(rec.seq) for rec in SeqIO.parse(TRAIN_SEQ, 'fasta')}\ntest_sequences  = {rec.id: str(rec.seq) for rec in SeqIO.parse(TEST_SEQ,  'fasta')}\n\nprint(f'Loaded {len(train_sequences)} train and {len(test_sequences)} test sequences')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:09:08.998142Z","iopub.execute_input":"2025-12-14T06:09:08.998316Z","iopub.status.idle":"2025-12-14T06:09:11.461835Z","shell.execute_reply.started":"2025-12-14T06:09:08.998301Z","shell.execute_reply":"2025-12-14T06:09:11.461186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Train dict:\", list(train_sequences.items())[0])\nprint(\"Test dict:\", list(test_sequences.items())[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:09:11.462497Z","iopub.execute_input":"2025-12-14T06:09:11.462669Z","iopub.status.idle":"2025-12-14T06:09:11.527154Z","shell.execute_reply.started":"2025-12-14T06:09:11.462654Z","shell.execute_reply":"2025-12-14T06:09:11.526361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ids = [i.split('|')[1] for i in train_sequences.keys()]\ntest_ids = list(test_sequences.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:09:11.527943Z","iopub.execute_input":"2025-12-14T06:09:11.528203Z","iopub.status.idle":"2025-12-14T06:09:11.556115Z","shell.execute_reply.started":"2025-12-14T06:09:11.528182Z","shell.execute_reply":"2025-12-14T06:09:11.555523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"train_ids[0:10]:\", train_ids[0:10])\nprint(\"test_ids[0:10]:\", test_ids[0:10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:09:11.556966Z","iopub.execute_input":"2025-12-14T06:09:11.557213Z","iopub.status.idle":"2025-12-14T06:09:11.567659Z","shell.execute_reply.started":"2025-12-14T06:09:11.557196Z","shell.execute_reply":"2025-12-14T06:09:11.566944Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 2: Feature extraction\n\n---","metadata":{}},{"cell_type":"code","source":"# Embeddings file paths\nESM_EMBEDDINGS = \"/kaggle/input/cafa6-esm2-650m-embedding/esm2_650M\"\nTRAIN_EMBEDDINGS = ESM_EMBEDDINGS + \"/train_sequences_emb.npy\"\nTEST_EMBEDDINGS = ESM_EMBEDDINGS + \"/testsuperset_emb.npy\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:09:11.569437Z","iopub.execute_input":"2025-12-14T06:09:11.569618Z","iopub.status.idle":"2025-12-14T06:09:11.578301Z","shell.execute_reply.started":"2025-12-14T06:09:11.569604Z","shell.execute_reply":"2025-12-14T06:09:11.577659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Load embeddings\nX_train = np.load(TRAIN_EMBEDDINGS)\nX_test = np.load(TEST_EMBEDDINGS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:09:11.578943Z","iopub.execute_input":"2025-12-14T06:09:11.579147Z","iopub.status.idle":"2025-12-14T06:09:16.928121Z","shell.execute_reply.started":"2025-12-14T06:09:11.579131Z","shell.execute_reply":"2025-12-14T06:09:16.927321Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"X_train shape:\", X_train.shape)\nprint(\"X_test shape:\", X_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:09:16.928985Z","iopub.execute_input":"2025-12-14T06:09:16.929213Z","iopub.status.idle":"2025-12-14T06:09:16.933387Z","shell.execute_reply.started":"2025-12-14T06:09:16.929195Z","shell.execute_reply":"2025-12-14T06:09:16.932805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=100, random_state=42)\nX_train_reduced = pca.fit_transform(X_train)\nX_test_reduced  = pca.transform(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:09:16.933983Z","iopub.execute_input":"2025-12-14T06:09:16.934229Z","iopub.status.idle":"2025-12-14T06:09:29.258819Z","shell.execute_reply.started":"2025-12-14T06:09:16.934212Z","shell.execute_reply":"2025-12-14T06:09:29.258177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"X_train_reduced shape:\", X_train_reduced.shape)\nprint(\"X_test_reduced shape:\", X_test_reduced.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:09:29.259524Z","iopub.execute_input":"2025-12-14T06:09:29.259861Z","iopub.status.idle":"2025-12-14T06:09:29.265023Z","shell.execute_reply.started":"2025-12-14T06:09:29.259837Z","shell.execute_reply":"2025-12-14T06:09:29.264320Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3: Label encoding and Training\n\n---","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom tqdm import tqdm\nimport lightgbm as lgb\n\nmlb_dict = {}\nlgbm_models = {}   # {aspect: {go_term: model}}\n\ntrain_terms_df = pd.read_csv(TRAIN_TERMS, sep=\"\\t\")\n\nfor aspect in ['P', 'C', 'F']:\n    print(f\"\\n========== Training LightGBM for aspect {aspect} ==========\")\n\n    ont_terms_df = train_terms_df[train_terms_df['aspect'] == aspect]\n\n    protein_terms = (\n        ont_terms_df\n        .groupby('EntryID')['term']\n        .apply(list)\n        .to_dict()\n    )\n\n    labels = [protein_terms.get(eid, []) for eid in train_ids]\n\n    mlb = MultiLabelBinarizer(sparse_output=True)\n    y_train = mlb.fit_transform(labels)\n\n    mlb_dict[aspect] = mlb\n\n    print(f\"y_train shape: {y_train.shape}\")\n\n    # ===== chọn top 50% GO terms =====\n    TOP_K = 1000   # số GO terms muốn dùng cho LightGBM\n    \n    y_dense = y_train.toarray()\n    term_freq = y_dense.sum(axis=0)   # shape: (n_terms,)\n    \n    # sort theo tần suất giảm dần\n    sorted_indices = np.argsort(term_freq)[::-1]\n    \n    # lấy top-K\n    selected_indices = sorted_indices[:TOP_K]\n    \n    print(f\"Selected {len(selected_indices)}/{len(term_freq)} GO terms (top-{TOP_K})\")\n\n    # ===== train LightGBM =====\n    models_aspect = {}\n\n    for idx in tqdm(selected_indices, desc=f\"LGBM-{aspect}\"):\n        y_i = y_dense[:, idx]\n        go_term = mlb.classes_[idx]\n\n        train_data = lgb.Dataset(X_train_reduced, label=y_i)\n\n        params = {\n            \"objective\": \"binary\",\n            \"metric\": \"binary_logloss\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"min_data_in_leaf\": 20,\n            \"verbosity\": -1\n        }\n\n        model = lgb.train(\n            params,\n            train_data,\n            num_boost_round=200\n        )\n\n        models_aspect[go_term] = model\n\n    lgbm_models[aspect] = models_aspect\n\n    print(f\"Finished training {len(models_aspect)} models for aspect {aspect}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:09:29.266316Z","iopub.execute_input":"2025-12-14T06:09:29.266538Z","iopub.status.idle":"2025-12-14T06:13:16.194322Z","shell.execute_reply.started":"2025-12-14T06:09:29.266516Z","shell.execute_reply":"2025-12-14T06:13:16.193337Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 4: Inference and Submission","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 5000\nsubmission_list = []\n\nfor i in tqdm(range(0, len(test_ids), BATCH_SIZE), desc=\"Predicting (LightGBM)\"):\n    batch_entry_ids = test_ids[i : i + BATCH_SIZE]\n    X_batch = X_test_reduced[i : i + BATCH_SIZE]\n\n    for aspect, models_aspect in lgbm_models.items():\n        for go_term, model in models_aspect.items():\n            probs = model.predict(X_batch)\n\n            for j, entry_id in enumerate(batch_entry_ids):\n                if probs[j] > 0.02:\n                    submission_list.append(\n                        (entry_id, go_term, round(float(probs[j]), 3))\n                    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:13:16.194845Z","iopub.status.idle":"2025-12-14T06:13:16.195077Z","shell.execute_reply.started":"2025-12-14T06:13:16.194972Z","shell.execute_reply":"2025-12-14T06:13:16.194983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df = pd.DataFrame(submission_list, columns=['Protein Id', 'GO Term Id', 'Prediction'])\nsubmission_df.to_csv('submission_no_limit.tsv', sep='\\t', index=False, header=False)\n\nprint(\"Applying 2000 prediction limit per protein...\")\nsubmission_df = submission_df.sort_values(by=['Protein Id', 'Prediction'], ascending=[True, False])\nfinal_submission_df = submission_df.groupby('Protein Id').head(2000).reset_index(drop=True)\nfinal_submission_df.to_csv('submission.tsv', sep='\\t', index=False, header=False)\n\nprint(\"\\nSubmission file 'submission.tsv' created successfully.\")\nprint(f\"Total predictions in final submission: {len(final_submission_df):,}\")\nprint(\"Submission DataFrame Head:\")\ndisplay(final_submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:13:16.196661Z","iopub.status.idle":"2025-12-14T06:13:16.197069Z","shell.execute_reply.started":"2025-12-14T06:13:16.196940Z","shell.execute_reply":"2025-12-14T06:13:16.196955Z"}},"outputs":[],"execution_count":null}]}